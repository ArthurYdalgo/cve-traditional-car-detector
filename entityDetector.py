import cv2
import sys
import numpy as np
from time import sleep
from scipy.spatial import ConvexHull, convex_hull_plot_2d
from shapely.geometry import Polygon

class EntityDetector(object):
    """
    Responsible for detecting entities in a determined area

    Parameters
    ----------
    footage_source : str or int
        The file name or index of video source. In case the source is a video, such file must be in the 'test footage' directory
    footage_fps : float
        Footage refresh rate
    minimum_area_ratio : float (from 0 to 1)
        In order to a detection to be validated, it needs a minimum area ratio (detection_rate by the search area, which is 0.35 (35 %) by default)
    center_displacement_tolerance : float (from 0 to 1)
        In order to a detection be validated, it's center needs to be within a defined tolerance (which is by default 0.6 (60 %))
    search_area : dictionary
        Dictionary that defines search area attributes.
        The dictionary must contain a 'position' key and a 'dimension' key, and
            each key contains a tuple of the desired x/y and width/height values
        e.g.: {'position':(715,475),'dimension':(150,35)}
    """

    def __init__(self, footage_source='road footage.mp4',footage_fps = 1/15, minimum_area_ratio = 0.35, center_displacement_tolerance = 0.6, search_area={'position': (570, 600), 'dimension': (160, 100)}):

        # EntityDetector changable properties
        self.footage_source = footage_source        
        self.center_displacement_tolerance = center_displacement_tolerance
        self.minimum_area_ratio= minimum_area_ratio
        self.footage_fps = footage_fps
        
        self.minimum_pixels = 3


        # Search area properties
        try:
            self.search_area_width = int(search_area['dimension'][0])
            self.search_area_height = int(search_area['dimension'][1])
            self.search_area_x = int(search_area['position'][0])
            self.search_area_y = int(search_area['position'][1])
        except:
            print('Invalid search area')
            sys.exit()


    def setup(self):
        """
        Responsible for instantiating footage
        """
        footage = self.getFootage()

        return footage


    def detect(self, display=True, console_output=True, detailed_detection=False, export=None):
        """
        Detects entities passing through search area.
        To exit, press q

        Parameters
        ----------
        display : bool
            In case the footage isn't needed, this parameter can be set to False
        console_output : bool
            In case console output isn't needed, this parameter can be set to False
        detailed_detection : bool
            In case detailed detection is desired, this parameter can be set to False

        """
        footage = self.setup()

        if(export):        
            export_footage_frames = []

        while(footage.isOpened()):
            ret, frame = footage.read()

            # Crops footage
            try:
                x_1 = int(self.search_area_x - self.search_area_width/2)
                if(x_1 < 0):
                    x_1 = 0

                x_2 = int(self.search_area_x + self.search_area_width/2)
                if(x_2 >= frame.shape[1]):
                    x_2 = frame.shape[1] - 1

                y_1 = int(self.search_area_y - self.search_area_height/2)
                if(y_1 < 0):
                    y_1 = 0

                y_2 = int(self.search_area_y + self.search_area_height/2)
                if(y_2 >= frame.shape[0]):
                    y_2 = frame.shape[0] - 1
                frame = frame[y_1:y_2,x_1:x_2]
            except:                
                break
            
            try:
                # New gray frame is created
                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)            
            except:
                break
            edge_detection = cv2.Canny(gray_frame, 60, 100)

            # Each line and column is scanned for continuous pixels which represent straight lines.
            #   minimum continuous pixels to be considered a line is defined in the class initializer
            shape = edge_detection.shape

            horizontal_dimension = shape[0]
            vertical_dimension = shape[1]

            horizontal_lines = []
            vertical_lines = []

            # Scans for horizontal lines
            for line in range(horizontal_dimension):
                x_pixel = None
                for pixel in range(vertical_dimension):

                    # If finds a pixel of an edge detection...
                    if(edge_detection[line][pixel]):
                        if(x_pixel is None):
                            x_pixel = pixel
                    else:
                        if(x_pixel and pixel - self.minimum_pixels >= x_pixel):
                            horizontal_lines.append((line,x_pixel,pixel))

                        x_pixel = None


            # Scans for vertical lines
            for pixel in range(vertical_dimension):
                y_pixel = None
                for line in range(horizontal_dimension):

                    # If finds a pixel of an edge detection...
                    if(edge_detection[line][pixel]):
                        if(y_pixel is None):
                            y_pixel = line
                    else:
                        if(y_pixel and line - self.minimum_pixels >= y_pixel):
                            vertical_lines.append((pixel,y_pixel,line))

                        y_pixel = None

            

            # Creates frames for detailed detection
            detection_frame = frame.copy()
            if(detailed_detection):
                frame_with_lines = frame.copy()
                frame_with_points = frame.copy()
                hull_frame = frame.copy()
            
            points = []
            x_sum = 0
            y_sum = 0

            # Gets coordinates of each line center point
            for line in vertical_lines:
                point = (line[0],int((line[2]+line[1])/2))  
                x_sum += point[0]
                y_sum += point[1]         
                points.append(point)
                if(detailed_detection):
                    cv2.line(frame_with_lines,(line[0],line[1]),(line[0],line[2]),(200,0,200),1)                    
                    cv2.circle(frame_with_points, point, radius=1, color=(0, 0, 255), thickness=-1)
                

            for line in horizontal_lines:
                point = (int((line[2]+line[1])/2),line[0])
                x_sum += point[0]
                y_sum += point[1]
                points.append(point)
                if(detailed_detection):
                    cv2.line(frame_with_lines,(line[1],line[0]),(line[2],line[0]),(0,0,200),1)
                    cv2.circle(frame_with_points, point, radius=1, color=(0, 0, 255), thickness=-1)

            # Converts points to np array
            points = np.array(points)

            hull_points = []

            # Convex Hull algorithm is used to detect outer points of edge detection
            try:
                hull = ConvexHull(points)   
                for point in hull.vertices: 
                    x = points[point][0]
                    y = points[point][1]
                    hull_points.append((x,y)) 
                    
                    # Hull points are drawn on hull frame, in case detailed detection is enabled
                    if(detailed_detection):                                                               
                        cv2.circle(hull_frame, (x, y), radius=2, color=(255, 0, 0), thickness=-1)                

                # Calculates overall center of detection            
                x_center = x_sum / len(points)
                y_center = y_sum / len(points)
                
                # Total frame area
                frame_area = self.search_area_height*self.search_area_width

                # Calculates hull area and hull area ration
                hull_area = Polygon(hull_points).area
                hull_area_ratio = hull_area/frame_area
                
                # Which is compared to minimum ratio. Center displacement tolance is also checked to make
                #    sure the detection is not off center
                if(hull_area_ratio > self.minimum_area_ratio and self.isCentered(x_center,y_center)):

                    if(console_output):
                        print("Carro encontrado")

                    # Extreme points are used to draw a border around convex hull
                    border_x_1 = self.search_area_width
                    border_x_2 = 0
                    border_y_1 = self.search_area_height
                    border_y_2 = 0
                    for point in hull_points:
                        if(border_x_1 > point[0]):                            
                            border_x_1 = point[0]
                        if(border_x_2 < point[0]):
                            border_x_2 = point[0]
                        if(border_y_1 > point[1]):
                            border_y_1 = point[1]
                        if(border_y_2 < point[1]):
                            border_y_2 = point[1]

                    cv2.rectangle(detection_frame,(int(border_x_1),int(border_y_1)),(int(border_x_2),int(border_y_2)),(0,0,200),2)

                    # A text and a circle are also drawn on detection
                    cv2.circle(detection_frame, (int(x_center), int(y_center)), radius=3, color=(255, 0, 0), thickness=-1)     
                    cv2.putText(detection_frame,"Carro detectado",(20,15) ,cv2.FONT_HERSHEY_SIMPLEX,0.5, (200, 0, 200), 2)
                    

            except:                
                pass

            frame = cv2.resize(detection_frame, (500, 250)) 

            # Displays footage and detection (can be disable)
            if(display):
                # Frame(s) displayed to the user, if display option is enabled
                if(detailed_detection):
                    edge_detection_frame= np.hstack((gray_frame,edge_detection))
                    detailed_frame = np.hstack((frame,frame_with_lines,frame_with_points,hull_frame,detection_frame))
                    edge_detection_frame = cv2.resize(edge_detection_frame, (500, 250)) 
                    detailed_frame = cv2.resize(detailed_frame, (1200, 250)) 
                    cv2.imshow('Detailed Detection', detailed_frame)
                    cv2.imshow('Edge Detection Frame', edge_detection_frame)
                    
                else:                    
                    cv2.imshow('CVE Footage', frame)  

                # Footage refresh rate
                sleep(self.footage_fps)     

            if(export):
                export_footage_frames.append(frame)

            # Press q to exit
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        if(export):
            
            exporter = cv2.VideoWriter("{}.avi".format(export), cv2.VideoWriter_fourcc(*'DIVX'),1/self.footage_fps,(500 , 250))
            for export_frame in export_footage_frames:
                exporter.write(export_frame)
            exporter.release()

        footage.release()
        cv2.destroyAllWindows()

        return

    def isCentered(self,x, y):
        """
        Checks if a point is within center displacement tolerance
        """
        frame_x_center = self.search_area_width / 2
        frame_y_center = self.search_area_height / 2

        x_displacement = ((abs(x - frame_x_center))/frame_x_center)
        y_displacement = ((abs(y - frame_y_center))/frame_y_center)
        
        if(x_displacement > self.center_displacement_tolerance or y_displacement > self.center_displacement_tolerance):
            return False
        return True

    def getFootage(self):
        """
        Instantiates OpenCV footage based on source
        """

        # OpenCV's "VideoCapture()" method's parameter is either an index (which is related to a connected source) or a filename
        #
        # For more info regarding cv2.VideoCapture(), go to: https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html#a57c0e81e83e60f36c83027dc2a188e80
        footage = cv2.VideoCapture(self.footage_source)

        # Checks wether or not the given source is a valid one
        if footage is None or not footage.isOpened():
            print('Invalid source')
            sys.exit()

        return footage
